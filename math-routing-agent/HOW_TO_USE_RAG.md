# How to Use the RAG Pipeline

## âœ… RAG Pipeline is Now Integrated!

Your app now has a complete RAG pipeline that works automatically alongside your Math Professor AI app.

## ğŸš€ Quick Start

### 1. Start the App

```bash
cd math-routing-agent
npm run dev
```

This starts:
- âœ… React app on `http://localhost:5173`
- âœ… Python chunking API on `http://localhost:5000`

### 2. Verify RAG is Working

#### Step 1: Upload a Document

1. Open `http://localhost:5173`
2. Click the upload button (ğŸ“)
3. Upload a PDF or TXT file with math content

**What happens:**
- Document is chunked using Python API
- Chunks are stored in RAG pipeline
- Questions are extracted
- You'll see: `âœ… Document processed! I've stored X chunks in the RAG pipeline`

#### Step 2: Ask a Question

1. Ask a question related to the uploaded document
2. The RAG pipeline will:
   - Search for relevant chunks
   - Use chunks as context for the answer
   - Generate answer using document context

**What you'll see:**
- Console log: `ğŸ” RAG: Retrieved X relevant chunks from Y total chunks`
- Answer uses context from your document

## ğŸ” How to Verify It's Working

### Check Console Logs

Open browser DevTools (F12) â†’ Console tab:

**When uploading a document:**
```
âœ… RAG Pipeline: Stored X chunks from filename.pdf
```

**When asking a question:**
```
ğŸ” RAG: Retrieved X relevant chunks from Y total chunks
```

### Check Browser Storage

1. Open DevTools (F12)
2. Go to Application â†’ Local Storage
3. Look for `math_rag_chunks` key
4. You should see your stored chunks

### Test the Flow

1. **Upload a document** with math content (e.g., "The Pythagorean theorem states that aÂ² + bÂ² = cÂ²...")
2. **Ask a question** like "What is the Pythagorean theorem?"
3. **Check the answer** - it should reference your document content

## ğŸ“Š RAG Pipeline Flow

### Document Upload Flow

```
User uploads document
    â†“
Extract text
    â†“
Chunk document (Python API)
    â†“
Generate embeddings
    â†“
Store in vector store (localStorage)
    â†“
Extract questions
    â†“
Display to user
```

### Question Answering Flow

```
User asks question
    â†“
Guardrail check (is it math?)
    â†“
RAG: Retrieve relevant chunks
    â†“
If chunks found: Use as context
    â†“
If not: Fallback to keyword search
    â†“
If still not: Use web search
    â†“
Generate answer with context
    â†“
Display answer
```

## ğŸ¯ How It Works

### 1. Document Processing

When you upload a document:
- Text is extracted
- Document is sent to Python chunking API
- Chunks are created with semantic boundaries
- Embeddings are generated for each chunk
- Chunks are stored in localStorage (vector store)

### 2. Question Retrieval

When you ask a question:
- Question embedding is generated
- Similarity search finds relevant chunks
- Top 3 most relevant chunks are retrieved
- Chunks are used as context for the LLM

### 3. Answer Generation

The LLM receives:
- Your question
- Relevant chunks from RAG pipeline
- Instructions to use context

The answer is generated using your document content!

## ğŸ“ Example Usage

### Upload a Document

1. Create a text file with math content:
   ```
   The Pythagorean theorem states that in a right triangle, 
   aÂ² + bÂ² = cÂ², where c is the hypotenuse.
   ```

2. Upload it through the app

3. You'll see: `âœ… Document processed! I've stored X chunks`

### Ask Questions

Ask questions related to your document:
- "What is the Pythagorean theorem?"
- "Explain the formula aÂ² + bÂ² = cÂ²"
- "What is the hypotenuse?"

The answers will use context from your uploaded document!

## ğŸ”§ Configuration

### Environment Variables

Make sure you have `.env` file:

```env
VITE_API_KEY=your_gemini_api_key
VITE_CHUNKING_API_URL=http://localhost:5000/chunk
```

### Python API

The Python API should be running:
```bash
npm run dev  # Starts both React and Python API
```

## ğŸ› Troubleshooting

### RAG Not Working?

1. **Check Python API is running:**
   ```bash
   curl http://localhost:5000/health
   ```
   Should return: `{"status":"healthy","chunking_available":true}`

2. **Check console for errors:**
   - Open DevTools â†’ Console
   - Look for error messages

3. **Check environment variables:**
   - Verify `VITE_CHUNKING_API_URL` is set
   - Check `.env` file exists

### Chunks Not Stored?

1. **Check localStorage:**
   - DevTools â†’ Application â†’ Local Storage
   - Look for `math_rag_chunks`

2. **Check console logs:**
   - Should see: `âœ… RAG Pipeline: Stored X chunks`

### Chunks Not Retrieved?

1. **Check if chunks exist:**
   - DevTools â†’ Application â†’ Local Storage
   - Verify chunks are stored

2. **Check question relevance:**
   - Question should be related to uploaded document
   - Try asking about specific content from document

## ğŸ“Š Check RAG Status

You can check RAG pipeline status in code:

```typescript
import { RAGService } from './services/ragService';

const stats = RAGService.getStats();
console.log(`Total chunks: ${stats.totalChunks}`);
console.log(`Documents: ${stats.documents.join(', ')}`);
```

## ğŸ‰ You're All Set!

The RAG pipeline is now fully integrated and working automatically:

- âœ… Documents are chunked when uploaded
- âœ… Chunks are stored in vector store
- âœ… Questions retrieve relevant chunks
- âœ… Answers use RAG context

Just upload documents and ask questions - the RAG pipeline handles everything!

## ğŸ“š Next Steps

1. **Test with multiple documents** - Upload several documents and ask questions
2. **Check retrieval quality** - See how well chunks are retrieved
3. **Improve embeddings** - For production, use better embedding models
4. **Add vector database** - Replace localStorage with a real vector DB

## ğŸ”— Related Files

- `src/services/ragService.ts` - RAG pipeline service
- `src/hooks/useRAG.ts` - React hook for RAG
- `src/components/RAGStatus.tsx` - Status component
- `RAG_INTEGRATION_GUIDE.md` - Detailed integration guide

